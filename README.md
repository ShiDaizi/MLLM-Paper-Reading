# MLLM-Paper-Reading
This is a paper reading repository for recording my list of read papers.

## 📖 Table of Contents
- [MLLM-Paper-Reading](#mllm-paper-reading)
  - [📖 Table of Contents](#-table-of-contents)
  - [Image LLMs](#image-llms)

## Image LLMs
- [x] **LLaVA-Critic**: Learning to Evaluate Multimodal Models. CVPR2025 [Paper](https://arxiv.org/abs/2410.02712) [Page](https://llava-vl.github.io/blog/2024-10-03-llava-critic/)
  
  This paper introduces LLaVA-Critic, the first open-source large multimodal model (LMM) designed as a generalist evaluator for assessing performance across various multimodal tasks. Trained on a high-quality critic instruction-following dataset comprising 46k images and 113k evaluation samples (including both pointwise and pairwise settings), LLaVA-Critic demonstrates effectiveness in two key areas. Firstly, as an LMM-as-a-Judge, it provides reliable evaluation scores, achieving performance on par with or surpassing GPT models across multiple benchmarks, with high correlation with GPT-4o in instance-level scoring and model-level ranking. Secondly, in preference learning, it generates effective reward signals for iterative Direct Preference Optimization (DPO), outperforming human feedback-based reward models in enhancing model alignment capabilities. The model, built by fine-tuning LLaVA-OneVision, preserves original visual capabilities while offering a cost-effective, open-source alternative to commercial evaluators, supporting tasks like visual chat, detailed description, and hallucination detection. This work highlights the potential of open-source LMMs in self-critique and evaluation, paving the way for scalable alignment feedback mechanisms.
- [x] **Img-Diff**: Contrastive Data Synthesis for Multimodal Large Language Models. CVPR2025 [Paper](https://arxiv.org/abs/2408.04594) [Code](https://github.com/modelscope/data-juicer/tree/ImgDiff)

  This paper introduces Img-Diff, a novel contrastive data synthesis method designed to enhance fine-grained image recognition capabilities in Multimodal Large Language Models (MLLMs). The approach generates high-quality datasets of "object replacement" samples by creating pairs of similar images with subtle object variations, identifying difference regions via a Difference Area Generator, and producing precise difference descriptions using a Difference Captions Generator. The resulting Img-Diff dataset, comprising 12,688 instances, effectively improves MLLMs' performance when used for fine-tuning. Experimental results show that fine-tuned models (e.g., LLaVA-1.5-7B, MGM-7B, InternVL2-8B) achieve significant gains on image difference benchmarks, with MGM-7B surpassing state-of-the-art models like GPT-4V and Gemini by up to 12 points on the MMVP benchmark. Additionally, the models demonstrate an average improvement of 3.06% across eight MLLM benchmarks, validating the dataset's ability to enhance both image difference recognition and overall visual understanding. The dataset exhibits high quality (over 70% accurate difference descriptions) and diversity (covering 1,203 object categories), offering valuable insights for multimodal data synthesis.
- [x] **FlashSloth**: Lightning Multimodal Large Language Models via Embedded Visual Compression. CVPR2025 [Paper](https://arxiv.org/abs/2412.04317) [Code](https://github.com/codefanw/FlashSloth)
  
  This paper presents FlashSloth, a powerful and fast tiny multimodal large language model (MLLM) that achieves a superior balance between performance and efficiency by introducing embedded visual compression designs, specifically Spatial Attention Pooling (SAP) to capture visually salient semantics and an Embedded Query (EmbQ) module to grasp instruction-related image information, thereby greatly reducing the number of visual tokens, training memory, computation complexity, and response time while retaining high performance on various vision-language tasks compared to advanced tiny MLLMs like InternVL2, MiniCPM-V2, and Qwen2-VL.
  **This paper modifies the image token embedding method in a tricky way, and through pre-training plus SFT, it outperforms other models in terms of performance, speed, and memory usage. However, it doesn't mention which datasets were used for training??**
- [x] **BlueLM-V-3B**: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices. CVPR2025 [Paper](https://arxiv.org/abs/2411.10640v1)
  
  BlueLM-V-3B is a 3B-parameter mobile-optimized multimodal large language model that achieves superior performance (topping OpenCompass among ≤4B models with 66.1 points and outperforming some 8B models) and efficiency (2.2GB memory, 24.4 token/s on MediaTek Dimensity 9300) through algorithm-system co-design, including relaxed aspect ratio matching, token downsampling, mixed-precision quantization, and two-stage training on 2.5M pretraining and 645M fine-tuning image-text pairs (both open-source and in-house data).
- [x] **Insight-V**: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models. CVPR2025 [Paper](https://arxiv.org/abs/2411.14432) [Code](https://github.com/dongyh20/Insight-V)
  
  Llama-VID, a video understanding large language model based on the Llama architecture, achieves superior performance across 14 video understanding benchmarks (with an average 5.3% improvement in VideoQA) and enhanced efficiency (2.1x faster inference and 40% lower memory usage) through key innovations like dynamic video tokenization (adjusting tokens based on content complexity) and temporal-aware attention (strengthening long-video temporal modeling), trained on 8.7M video-text pairs for pre-training and 1.2M instruction data for fine-tuning.
- [x] **Critic-V**: VLM Critics Help Catch VLM Errors in Multimodal Reasoning. CVPR2025 [Paper](https://arxiv.org/abs/2411.18203)

  Critic-V is a novel framework inspired by the Actor-Critic paradigm designed to enhance the multimodal reasoning capabilities of Vision-Language Models (VLMs) by addressing issues like hallucinated image understandings and unrefined reasoning paths. It comprises two core components: the Reasoner, which generates reasoning paths from visual and textual inputs using dynamic text prompts that evolve iteratively, and the Critic, which provides nuanced natural language feedback (instead of scalar rewards) to refine these paths, operating within a reinforcement learning framework. The Critic is trained via Direct Preference Optimization (DPO) on a dataset of 29,012 multimodal question-answer pairs, with critiques ranked using a Rule-based Reward (RBR) that combines Jaccard similarity (for error detection accuracy) and GPT-4o scores (for feedback quality). Evaluation results demonstrate that Critic-V significantly outperforms existing methods, including GPT-4V, on 5 out of 8 benchmarks, with notable improvements in mathematical reasoning (e.g., an 11.8% boost on MathVista for Qwen2-VL-7B) and strong performance across tasks like RealWorldQA and MMT-Bench, highlighting its effectiveness in enhancing VLM reliability for real-world applications such as autonomous driving and embodied intelligence.
- [x] **Mono-InternVL**: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training. CVPR2025 [Paper](https://arxiv.org/abs/2410.08202) [Code](https://internvl.github.io/blog/2024-10-10-Mono-InternVL/)

  Mono-InternVL是一种新型单体多模态大语言模型（MLLM），旨在解决现有单体 MLLMs 存在的不稳定优化和灾难性遗忘问题。其核心创新在于通过多模态混合专家（MMoE）结构将视觉专家嵌入预训练 LLM，并采用内生视觉预训练（EViP） 策略 —— 分三阶段（概念学习、语义学习、对齐学习）从噪声数据到高质量数据渐进式学习视觉知识。实验表明，Mono-InternVL 在 16 个多模态基准中的 13 个超越现有单体 MLLMs（如在 OCRBench 上比 Emu3 高 80 分），同时与模块化模型 InternVL-1.5 性能相当，但首 token 延迟降低 67%，为单体 MLLMs 的发展提供了新方向。
- [x] **DivPrune**: Diversity-based Visual Token Pruning for Large Multimodal Models. CVPR2025 [Paper](https://arxiv.org/abs/2503.02175) [Code](https://github.com/vbdi/divprune)

  DivPrune是一种基于Max-Min Diversity Problem (MMDP) 的视觉 token 剪枝方法，旨在解决大型多模态模型（LMMs）中视觉 token 冗余导致的高推理延迟问题。其核心是通过最大化保留 token 的多样性（即最大化最小 pairwise 距离）减少冗余，无需微调或校准数据。实验表明，DivPrune 在 16 个图像和视频语言数据集上实现最先进精度，尤其在高剪枝率（≥80%）下优势显著；同时降低 GPU 内存使用和端到端延迟，例如在 LLaVA 1.5-7B 上，剪枝 90% token 时 COCO 的 CIDEr 仅下降 12.7%，而现有方法下降 95%，且首 token 延迟降低 67%。
- [x] **ODE**: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models. CVPR2025 [Paper](https://arxiv.org/abs/2409.09318)

  ODE（Open-Set Dynamic Evaluation） 是一种开放集动态评估协议，旨在解决多模态大语言模型（MLLMs）幻觉评估中数据污染的问题。其核心是通过图结构建模现实世界的物体概念、属性及关联，基于四种分布标准（Standard、Long-tail、Random、Fictional） 生成动态样本，评估存在级和属性级幻觉。实验表明，ODE 生成的样本揭示了 MLLMs 更高的幻觉率（如 MiniGPT-4 在 ODE 上的 F1 分数比静态基准低 20% 以上），且生成数据可辅助模型微调（LLaVA-1.5 微调后幻觉率降低 7.6%），为 MLLMs 幻觉评估和优化提供了可靠方案。
- [x] Augmenting Multimodal LLMs with Self-Reflective Tokens for Knowledge-based Visual Question Answering. CVPR2025 [Paper](https://arxiv.org/abs/2411.16863) [Code](https://github.com/aimagelab/ReflectiVA)

  ReflectiVA是一种增强多模态大语言模型（MLLMs）的方法，通过引入自反射令牌（reflective tokens）（<RET>、<NORET>、<REL>、<NOREL>）实现对外部知识的动态利用。该模型采用两阶段两模型训练策略：首先训练文章内判别器区分同篇文档中的相关与不相关段落，再利用其标注数据结合混合数据集训练最终模型。实验表明，ReflectiVA 在Encyclopedic-VQA和InfoSeek等知识型视觉问答任务上显著优于现有方法（如在 Encyclopedic-VQA 单跳任务上准确率达 35.5%），同时保留在标准 MLLM 基准上的性能，为需外部知识的多模态任务提供了有效解决方案。
- [ ] **AGLA**: Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention. CVPR2025 [Paper](https://arxiv.org/abs/2406.12718) [Code](https://github.com/Lackel/AGLA)
- [ ] **ICT**: Image-Object Cross-Level Trusted Intervention for Mitigating Object Hallucination in Large Vision-Language Models. CVPR2025 [Paper](https://arxiv.org/abs/2411.15268v1)
- [ ] Can Large Vision-Language Models Correct Grounding Errors By Themselves? CVPR2025 [Paper](https://openreview.net/pdf?id=fO1xnmW8T6)
- [ ] **Molmo and PixMo**: Open Weights and Open Data for State-of-the-Art Vision-Language Models. CVPR2025 [Paper](https://arxiv.org/abs/2409.17146) [Page](https://molmo.allenai.org/blog)
- [ ] **Nullu**: Mitigating Object Hallucinations in Large Vision-Language Models via HalluSpace Projection. CVPR2025 [Paper](https://arxiv.org/abs/2412.13817) [Code](https://github.com/Ziwei-Zheng/Nullu)
- [ ] **HiRes-LLaVA**: Restoring Fragmentation Input in High-Resolution Large Vision-Language Models. CVPR2025 [Paper](https://arxiv.org/abs/2407.08706)
- [ ] Devils in Middle Layers of Large Vision-Language Models: Interpreting, Detecting and Mitigating Object Hallucinations via Attention Lens. CVPR2025 [Paper](https://arxiv.org/abs/2411.16724)
- [ ] **HoVLE**: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding. CVPR2025 [Paper](https://arxiv.org/abs/2412.16158)
- [ ] **VoCo-LLaMA**: Towards Vision Compression with Large Language Models. CVPR2025 [Paper](https://arxiv.org/abs/2406.12275v2) [Code](https://github.com/Yxxxb/VoCo-LLaMA?tab=readme-ov-file)
- [ ] Perception Tokens Enhance Visual Reasoning in Multimodal Language Models. CVPR2025 [Paper](https://arxiv.org/abs/2412.03548) [Page](https://aurora-perception.github.io/)
- [ ] **Florence-VL**: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion. CVPR2025 [Paper](https://arxiv.org/abs/2412.04424) [Page](https://jiuhaichen.github.io/florence-vl.github.io/)
- [ ] **ATP-LLaVA**: Adaptive Token Pruning for Large Vision Language Models. CVPR2025 [Paper](https://arxiv.org/abs/2412.00447) [Page](https://yxxxb.github.io/ATP-LLaVA-page/)
- [ ] Accelerating Multimodel Large Language Models by Searching Optimal Vision Token Reduction. CVPR2025 [Paper](https://arxiv.org/abs/2412.00556)
- [ ] **VisionZip**: Longer is Better but Not Necessary in Vision Language Models. CVPR2025 [Paper](https://arxiv.org/abs/2412.04467) [Code](https://github.com/dvlab-research/VisionZip)
