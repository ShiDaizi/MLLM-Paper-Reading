# MLLM-Paper-Reading
This is a paper reading repository for recording my list of read papers.

## ğŸ“– Table of Contents
- [MLLM-Paper-Reading](#mllm-paper-reading)
  - [ğŸ“– Table of Contents](#-table-of-contents)
  - [Image LLMs](#image-llms)

## Image LLMs
- [x] **LLaVA-Critic**: Learning to Evaluate Multimodal Models. CVPR2025 [Paper](https://arxiv.org/abs/2410.02712) [Page](https://llava-vl.github.io/blog/2024-10-03-llava-critic/)
  
  This paper introduces LLaVA-Critic, the first open-source large multimodal model (LMM) designed as a generalist evaluator for assessing performance across various multimodal tasks. Trained on a high-quality critic instruction-following dataset comprising 46k images and 113k evaluation samples (including both pointwise and pairwise settings), LLaVA-Critic demonstrates effectiveness in two key areas. Firstly, as an LMM-as-a-Judge, it provides reliable evaluation scores, achieving performance on par with or surpassing GPT models across multiple benchmarks, with high correlation with GPT-4o in instance-level scoring and model-level ranking. Secondly, in preference learning, it generates effective reward signals for iterative Direct Preference Optimization (DPO), outperforming human feedback-based reward models in enhancing model alignment capabilities. The model, built by fine-tuning LLaVA-OneVision, preserves original visual capabilities while offering a cost-effective, open-source alternative to commercial evaluators, supporting tasks like visual chat, detailed description, and hallucination detection. This work highlights the potential of open-source LMMs in self-critique and evaluation, paving the way for scalable alignment feedback mechanisms.
- [x] **Img-Diff**: Contrastive Data Synthesis for Multimodal Large Language Models. CVPR2025 [Paper](https://arxiv.org/abs/2408.04594) [Code](https://github.com/modelscope/data-juicer/tree/ImgDiff)

  This paper introduces Img-Diff, a novel contrastive data synthesis method designed to enhance fine-grained image recognition capabilities in Multimodal Large Language Models (MLLMs). The approach generates high-quality datasets of "object replacement" samples by creating pairs of similar images with subtle object variations, identifying difference regions via a Difference Area Generator, and producing precise difference descriptions using a Difference Captions Generator. The resulting Img-Diff dataset, comprising 12,688 instances, effectively improves MLLMs' performance when used for fine-tuning. Experimental results show that fine-tuned models (e.g., LLaVA-1.5-7B, MGM-7B, InternVL2-8B) achieve significant gains on image difference benchmarks, with MGM-7B surpassing state-of-the-art models like GPT-4V and Gemini by up to 12 points on the MMVP benchmark. Additionally, the models demonstrate an average improvement of 3.06% across eight MLLM benchmarks, validating the dataset's ability to enhance both image difference recognition and overall visual understanding. The dataset exhibits high quality (over 70% accurate difference descriptions) and diversity (covering 1,203 object categories), offering valuable insights for multimodal data synthesis.
- [x] **FlashSloth**: Lightning Multimodal Large Language Models via Embedded Visual Compression. CVPR2025 [Paper](https://arxiv.org/abs/2412.04317) [Code](https://github.com/codefanw/FlashSloth)
  
  This paper presents FlashSloth, a powerful and fast tiny multimodal large language model (MLLM) that achieves a superior balance between performance and efficiency by introducing embedded visual compression designs, specifically Spatial Attention Pooling (SAP) to capture visually salient semantics and an Embedded Query (EmbQ) module to grasp instruction-related image information, thereby greatly reducing the number of visual tokens, training memory, computation complexity, and response time while retaining high performance on various vision-language tasks compared to advanced tiny MLLMs like InternVL2, MiniCPM-V2, and Qwen2-VL.
  **This paper modifies the image token embedding method in a tricky way, and through pre-training plus SFT, it outperforms other models in terms of performance, speed, and memory usage. However, it doesn't mention which datasets were used for training??**
- [x] **BlueLM-V-3B**: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices. CVPR2025 [Paper](https://arxiv.org/abs/2411.10640v1)
  
  BlueLM-V-3B is a 3B-parameter mobile-optimized multimodal large language model that achieves superior performance (topping OpenCompass among â‰¤4B models with 66.1 points and outperforming some 8B models) and efficiency (2.2GB memory, 24.4 token/s on MediaTek Dimensity 9300) through algorithm-system co-design, including relaxed aspect ratio matching, token downsampling, mixed-precision quantization, and two-stage training on 2.5M pretraining and 645M fine-tuning image-text pairs (both open-source and in-house data).
- [x] **Insight-V**: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models. CVPR2025 [Paper](https://arxiv.org/abs/2411.14432) [Code](https://github.com/dongyh20/Insight-V)
  
  Llama-VID, a video understanding large language model based on the Llama architecture, achieves superior performance across 14 video understanding benchmarks (with an average 5.3% improvement in VideoQA) and enhanced efficiency (2.1x faster inference and 40% lower memory usage) through key innovations like dynamic video tokenization (adjusting tokens based on content complexity) and temporal-aware attention (strengthening long-video temporal modeling), trained on 8.7M video-text pairs for pre-training and 1.2M instruction data for fine-tuning.
- [x] **Critic-V**: VLM Critics Help Catch VLM Errors in Multimodal Reasoning. CVPR2025 [Paper](https://arxiv.org/abs/2411.18203)

  Critic-V is a novel framework inspired by the Actor-Critic paradigm designed to enhance the multimodal reasoning capabilities of Vision-Language Models (VLMs) by addressing issues like hallucinated image understandings and unrefined reasoning paths. It comprises two core components: the Reasoner, which generates reasoning paths from visual and textual inputs using dynamic text prompts that evolve iteratively, and the Critic, which provides nuanced natural language feedback (instead of scalar rewards) to refine these paths, operating within a reinforcement learning framework. The Critic is trained via Direct Preference Optimization (DPO) on a dataset of 29,012 multimodal question-answer pairs, with critiques ranked using a Rule-based Reward (RBR) that combines Jaccard similarity (for error detection accuracy) and GPT-4o scores (for feedback quality). Evaluation results demonstrate that Critic-V significantly outperforms existing methods, including GPT-4V, on 5 out of 8 benchmarks, with notable improvements in mathematical reasoning (e.g., an 11.8% boost on MathVista for Qwen2-VL-7B) and strong performance across tasks like RealWorldQA and MMT-Bench, highlighting its effectiveness in enhancing VLM reliability for real-world applications such as autonomous driving and embodied intelligence.
- [x] **Mono-InternVL**: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training. CVPR2025 [Paper](https://arxiv.org/abs/2410.08202) [Code](https://internvl.github.io/blog/2024-10-10-Mono-InternVL/)

  Mono-InternVLæ˜¯ä¸€ç§æ–°å‹å•ä½“å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMï¼‰ï¼Œæ—¨åœ¨è§£å†³ç°æœ‰å•ä½“ MLLMs å­˜åœ¨çš„ä¸ç¨³å®šä¼˜åŒ–å’Œç¾éš¾æ€§é—å¿˜é—®é¢˜ã€‚å…¶æ ¸å¿ƒåˆ›æ–°åœ¨äºé€šè¿‡å¤šæ¨¡æ€æ··åˆä¸“å®¶ï¼ˆMMoEï¼‰ç»“æ„å°†è§†è§‰ä¸“å®¶åµŒå…¥é¢„è®­ç»ƒ LLMï¼Œå¹¶é‡‡ç”¨å†…ç”Ÿè§†è§‰é¢„è®­ç»ƒï¼ˆEViPï¼‰ ç­–ç•¥ â€”â€” åˆ†ä¸‰é˜¶æ®µï¼ˆæ¦‚å¿µå­¦ä¹ ã€è¯­ä¹‰å­¦ä¹ ã€å¯¹é½å­¦ä¹ ï¼‰ä»å™ªå£°æ•°æ®åˆ°é«˜è´¨é‡æ•°æ®æ¸è¿›å¼å­¦ä¹ è§†è§‰çŸ¥è¯†ã€‚å®éªŒè¡¨æ˜ï¼ŒMono-InternVL åœ¨ 16 ä¸ªå¤šæ¨¡æ€åŸºå‡†ä¸­çš„ 13 ä¸ªè¶…è¶Šç°æœ‰å•ä½“ MLLMsï¼ˆå¦‚åœ¨ OCRBench ä¸Šæ¯” Emu3 é«˜ 80 åˆ†ï¼‰ï¼ŒåŒæ—¶ä¸æ¨¡å—åŒ–æ¨¡å‹ InternVL-1.5 æ€§èƒ½ç›¸å½“ï¼Œä½†é¦– token å»¶è¿Ÿé™ä½ 67%ï¼Œä¸ºå•ä½“ MLLMs çš„å‘å±•æä¾›äº†æ–°æ–¹å‘ã€‚
- [x] **DivPrune**: Diversity-based Visual Token Pruning for Large Multimodal Models. CVPR2025 [Paper](https://arxiv.org/abs/2503.02175) [Code](https://github.com/vbdi/divprune)

  DivPruneæ˜¯ä¸€ç§åŸºäºMax-Min Diversity Problem (MMDP) çš„è§†è§‰ token å‰ªææ–¹æ³•ï¼Œæ—¨åœ¨è§£å†³å¤§å‹å¤šæ¨¡æ€æ¨¡å‹ï¼ˆLMMsï¼‰ä¸­è§†è§‰ token å†—ä½™å¯¼è‡´çš„é«˜æ¨ç†å»¶è¿Ÿé—®é¢˜ã€‚å…¶æ ¸å¿ƒæ˜¯é€šè¿‡æœ€å¤§åŒ–ä¿ç•™ token çš„å¤šæ ·æ€§ï¼ˆå³æœ€å¤§åŒ–æœ€å° pairwise è·ç¦»ï¼‰å‡å°‘å†—ä½™ï¼Œæ— éœ€å¾®è°ƒæˆ–æ ¡å‡†æ•°æ®ã€‚å®éªŒè¡¨æ˜ï¼ŒDivPrune åœ¨ 16 ä¸ªå›¾åƒå’Œè§†é¢‘è¯­è¨€æ•°æ®é›†ä¸Šå®ç°æœ€å…ˆè¿›ç²¾åº¦ï¼Œå°¤å…¶åœ¨é«˜å‰ªæç‡ï¼ˆâ‰¥80%ï¼‰ä¸‹ä¼˜åŠ¿æ˜¾è‘—ï¼›åŒæ—¶é™ä½ GPU å†…å­˜ä½¿ç”¨å’Œç«¯åˆ°ç«¯å»¶è¿Ÿï¼Œä¾‹å¦‚åœ¨ LLaVA 1.5-7B ä¸Šï¼Œå‰ªæ 90% token æ—¶ COCO çš„ CIDEr ä»…ä¸‹é™ 12.7%ï¼Œè€Œç°æœ‰æ–¹æ³•ä¸‹é™ 95%ï¼Œä¸”é¦– token å»¶è¿Ÿé™ä½ 67%ã€‚
- [x] **ODE**: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models. CVPR2025 [Paper](https://arxiv.org/abs/2409.09318)

  ODEï¼ˆOpen-Set Dynamic Evaluationï¼‰ æ˜¯ä¸€ç§å¼€æ”¾é›†åŠ¨æ€è¯„ä¼°åè®®ï¼Œæ—¨åœ¨è§£å†³å¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰å¹»è§‰è¯„ä¼°ä¸­æ•°æ®æ±¡æŸ“çš„é—®é¢˜ã€‚å…¶æ ¸å¿ƒæ˜¯é€šè¿‡å›¾ç»“æ„å»ºæ¨¡ç°å®ä¸–ç•Œçš„ç‰©ä½“æ¦‚å¿µã€å±æ€§åŠå…³è”ï¼ŒåŸºäºå››ç§åˆ†å¸ƒæ ‡å‡†ï¼ˆStandardã€Long-tailã€Randomã€Fictionalï¼‰ ç”ŸæˆåŠ¨æ€æ ·æœ¬ï¼Œè¯„ä¼°å­˜åœ¨çº§å’Œå±æ€§çº§å¹»è§‰ã€‚å®éªŒè¡¨æ˜ï¼ŒODE ç”Ÿæˆçš„æ ·æœ¬æ­ç¤ºäº† MLLMs æ›´é«˜çš„å¹»è§‰ç‡ï¼ˆå¦‚ MiniGPT-4 åœ¨ ODE ä¸Šçš„ F1 åˆ†æ•°æ¯”é™æ€åŸºå‡†ä½ 20% ä»¥ä¸Šï¼‰ï¼Œä¸”ç”Ÿæˆæ•°æ®å¯è¾…åŠ©æ¨¡å‹å¾®è°ƒï¼ˆLLaVA-1.5 å¾®è°ƒåå¹»è§‰ç‡é™ä½ 7.6%ï¼‰ï¼Œä¸º MLLMs å¹»è§‰è¯„ä¼°å’Œä¼˜åŒ–æä¾›äº†å¯é æ–¹æ¡ˆã€‚
- [x] Augmenting Multimodal LLMs with Self-Reflective Tokens for Knowledge-based Visual Question Answering. CVPR2025 [Paper](https://arxiv.org/abs/2411.16863) [Code](https://github.com/aimagelab/ReflectiVA)

  ReflectiVAæ˜¯ä¸€ç§å¢å¼ºå¤šæ¨¡æ€å¤§è¯­è¨€æ¨¡å‹ï¼ˆMLLMsï¼‰çš„æ–¹æ³•ï¼Œé€šè¿‡å¼•å…¥è‡ªåå°„ä»¤ç‰Œï¼ˆreflective tokensï¼‰ï¼ˆ<RET>ã€<NORET>ã€<REL>ã€<NOREL>ï¼‰å®ç°å¯¹å¤–éƒ¨çŸ¥è¯†çš„åŠ¨æ€åˆ©ç”¨ã€‚è¯¥æ¨¡å‹é‡‡ç”¨ä¸¤é˜¶æ®µä¸¤æ¨¡å‹è®­ç»ƒç­–ç•¥ï¼šé¦–å…ˆè®­ç»ƒæ–‡ç« å†…åˆ¤åˆ«å™¨åŒºåˆ†åŒç¯‡æ–‡æ¡£ä¸­çš„ç›¸å…³ä¸ä¸ç›¸å…³æ®µè½ï¼Œå†åˆ©ç”¨å…¶æ ‡æ³¨æ•°æ®ç»“åˆæ··åˆæ•°æ®é›†è®­ç»ƒæœ€ç»ˆæ¨¡å‹ã€‚å®éªŒè¡¨æ˜ï¼ŒReflectiVA åœ¨Encyclopedic-VQAå’ŒInfoSeekç­‰çŸ¥è¯†å‹è§†è§‰é—®ç­”ä»»åŠ¡ä¸Šæ˜¾è‘—ä¼˜äºç°æœ‰æ–¹æ³•ï¼ˆå¦‚åœ¨ Encyclopedic-VQA å•è·³ä»»åŠ¡ä¸Šå‡†ç¡®ç‡è¾¾ 35.5%ï¼‰ï¼ŒåŒæ—¶ä¿ç•™åœ¨æ ‡å‡† MLLM åŸºå‡†ä¸Šçš„æ€§èƒ½ï¼Œä¸ºéœ€å¤–éƒ¨çŸ¥è¯†çš„å¤šæ¨¡æ€ä»»åŠ¡æä¾›äº†æœ‰æ•ˆè§£å†³æ–¹æ¡ˆã€‚
- [ ] **AGLA**: Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention. CVPR2025 [Paper](https://arxiv.org/abs/2406.12718) [Code](https://github.com/Lackel/AGLA)
- [ ] **ICT**: Image-Object Cross-Level Trusted Intervention for Mitigating Object Hallucination in Large Vision-Language Models. CVPR2025 [Paper](https://arxiv.org/abs/2411.15268v1)
- [ ] Can Large Vision-Language Models Correct Grounding Errors By Themselves? CVPR2025 [Paper](https://openreview.net/pdf?id=fO1xnmW8T6)
- [ ] **Molmo and PixMo**: Open Weights and Open Data for State-of-the-Art Vision-Language Models. CVPR2025 [Paper](https://arxiv.org/abs/2409.17146) [Page](https://molmo.allenai.org/blog)
- [ ] **Nullu**: Mitigating Object Hallucinations in Large Vision-Language Models via HalluSpace Projection. CVPR2025 [Paper](https://arxiv.org/abs/2412.13817) [Code](https://github.com/Ziwei-Zheng/Nullu)
- [ ] **HiRes-LLaVA**: Restoring Fragmentation Input in High-Resolution Large Vision-Language Models. CVPR2025 [Paper](https://arxiv.org/abs/2407.08706)
- [ ] Devils in Middle Layers of Large Vision-Language Models: Interpreting, Detecting and Mitigating Object Hallucinations via Attention Lens. CVPR2025 [Paper](https://arxiv.org/abs/2411.16724)
- [ ] **HoVLE**: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding. CVPR2025 [Paper](https://arxiv.org/abs/2412.16158)
- [ ] **VoCo-LLaMA**: Towards Vision Compression with Large Language Models. CVPR2025 [Paper](https://arxiv.org/abs/2406.12275v2) [Code](https://github.com/Yxxxb/VoCo-LLaMA?tab=readme-ov-file)
- [ ] Perception Tokens Enhance Visual Reasoning in Multimodal Language Models. CVPR2025 [Paper](https://arxiv.org/abs/2412.03548) [Page](https://aurora-perception.github.io/)
- [ ] **Florence-VL**: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion. CVPR2025 [Paper](https://arxiv.org/abs/2412.04424) [Page](https://jiuhaichen.github.io/florence-vl.github.io/)
- [ ] **ATP-LLaVA**: Adaptive Token Pruning for Large Vision Language Models. CVPR2025 [Paper](https://arxiv.org/abs/2412.00447) [Page](https://yxxxb.github.io/ATP-LLaVA-page/)
- [ ] Accelerating Multimodel Large Language Models by Searching Optimal Vision Token Reduction. CVPR2025 [Paper](https://arxiv.org/abs/2412.00556)
- [ ] **VisionZip**: Longer is Better but Not Necessary in Vision Language Models. CVPR2025 [Paper](https://arxiv.org/abs/2412.04467) [Code](https://github.com/dvlab-research/VisionZip)
