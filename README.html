<!DOCTYPE html>
        <html>
        <head>
            <meta charset="UTF-8">
            <title>MLLM-Paper-Reading</title>
            <style>
/* From extension vscode.github */
/*---------------------------------------------------------------------------------------------
 *  Copyright (c) Microsoft Corporation. All rights reserved.
 *  Licensed under the MIT License. See License.txt in the project root for license information.
 *--------------------------------------------------------------------------------------------*/

.vscode-dark img[src$=\#gh-light-mode-only],
.vscode-light img[src$=\#gh-dark-mode-only],
.vscode-high-contrast:not(.vscode-high-contrast-light) img[src$=\#gh-light-mode-only],
.vscode-high-contrast-light img[src$=\#gh-dark-mode-only] {
	display: none;
}

</style>
            
        <link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/markdown.css">
<link rel="stylesheet" href="https://cdn.jsdelivr.net/gh/Microsoft/vscode/extensions/markdown-language-features/media/highlight.css">
<style>
            body {
                font-family: -apple-system, BlinkMacSystemFont, 'Segoe WPC', 'Segoe UI', system-ui, 'Ubuntu', 'Droid Sans', sans-serif;
                font-size: 14px;
                line-height: 1.6;
            }
        </style>
        <style>
.task-list-item {
    list-style-type: none;
}

.task-list-item-checkbox {
    margin-left: -20px;
    vertical-align: middle;
    pointer-events: none;
}
</style>
<style>
:root {
  --color-note: #0969da;
  --color-tip: #1a7f37;
  --color-warning: #9a6700;
  --color-severe: #bc4c00;
  --color-caution: #d1242f;
  --color-important: #8250df;
}

</style>
<style>
@media (prefers-color-scheme: dark) {
  :root {
    --color-note: #2f81f7;
    --color-tip: #3fb950;
    --color-warning: #d29922;
    --color-severe: #db6d28;
    --color-caution: #f85149;
    --color-important: #a371f7;
  }
}

</style>
<style>
.markdown-alert {
  padding: 0.5rem 1rem;
  margin-bottom: 16px;
  color: inherit;
  border-left: .25em solid #888;
}

.markdown-alert>:first-child {
  margin-top: 0
}

.markdown-alert>:last-child {
  margin-bottom: 0
}

.markdown-alert .markdown-alert-title {
  display: flex;
  font-weight: 500;
  align-items: center;
  line-height: 1
}

.markdown-alert .markdown-alert-title .octicon {
  margin-right: 0.5rem;
  display: inline-block;
  overflow: visible !important;
  vertical-align: text-bottom;
  fill: currentColor;
}

.markdown-alert.markdown-alert-note {
  border-left-color: var(--color-note);
}

.markdown-alert.markdown-alert-note .markdown-alert-title {
  color: var(--color-note);
}

.markdown-alert.markdown-alert-important {
  border-left-color: var(--color-important);
}

.markdown-alert.markdown-alert-important .markdown-alert-title {
  color: var(--color-important);
}

.markdown-alert.markdown-alert-warning {
  border-left-color: var(--color-warning);
}

.markdown-alert.markdown-alert-warning .markdown-alert-title {
  color: var(--color-warning);
}

.markdown-alert.markdown-alert-tip {
  border-left-color: var(--color-tip);
}

.markdown-alert.markdown-alert-tip .markdown-alert-title {
  color: var(--color-tip);
}

.markdown-alert.markdown-alert-caution {
  border-left-color: var(--color-caution);
}

.markdown-alert.markdown-alert-caution .markdown-alert-title {
  color: var(--color-caution);
}

</style>
        
        </head>
        <body class="vscode-body vscode-light">
            <h1 id="mllm-paper-reading">MLLM-Paper-Reading</h1>
<p>This is a paper reading repository for recording my list of read papers.</p>
<h2 id="image-llms">Image LLMs</h2>
<ul class="contains-task-list">
<li class="task-list-item enabled">
<p><input class="task-list-item-checkbox" checked=""type="checkbox"> <strong>LLaVA-Critic</strong>: Learning to Evaluate Multimodal Models. CVPR2025 <a href="https://arxiv.org/abs/2410.02712">Paper</a> <a href="https://llava-vl.github.io/blog/2024-10-03-llava-critic/">Page</a></p>
<p>This paper introduces LLaVA-Critic, the first open-source large multimodal model (LMM) designed as a generalist evaluator for assessing performance across various multimodal tasks. Trained on a high-quality critic instruction-following dataset comprising 46k images and 113k evaluation samples (including both pointwise and pairwise settings), LLaVA-Critic demonstrates effectiveness in two key areas. Firstly, as an LMM-as-a-Judge, it provides reliable evaluation scores, achieving performance on par with or surpassing GPT models across multiple benchmarks, with high correlation with GPT-4o in instance-level scoring and model-level ranking. Secondly, in preference learning, it generates effective reward signals for iterative Direct Preference Optimization (DPO), outperforming human feedback-based reward models in enhancing model alignment capabilities. The model, built by fine-tuning LLaVA-OneVision, preserves original visual capabilities while offering a cost-effective, open-source alternative to commercial evaluators, supporting tasks like visual chat, detailed description, and hallucination detection. This work highlights the potential of open-source LMMs in self-critique and evaluation, paving the way for scalable alignment feedback mechanisms.</p>
</li>
<li class="task-list-item enabled">
<p><input class="task-list-item-checkbox" checked=""type="checkbox"> <strong>Img-Diff</strong>: Contrastive Data Synthesis for Multimodal Large Language Models. CVPR2025 <a href="https://arxiv.org/abs/2408.04594">Paper</a> <a href="https://github.com/modelscope/data-juicer/tree/ImgDiff">Code</a></p>
</li>
<li>
<p>This paper introduces Img-Diff, a novel contrastive data synthesis method designed to enhance fine-grained image recognition capabilities in Multimodal Large Language Models (MLLMs). The approach generates high-quality datasets of &quot;object replacement&quot; samples by creating pairs of similar images with subtle object variations, identifying difference regions via a Difference Area Generator, and producing precise difference descriptions using a Difference Captions Generator. The resulting Img-Diff dataset, comprising 12,688 instances, effectively improves MLLMs' performance when used for fine-tuning. Experimental results show that fine-tuned models (e.g., LLaVA-1.5-7B, MGM-7B, InternVL2-8B) achieve significant gains on image difference benchmarks, with MGM-7B surpassing state-of-the-art models like GPT-4V and Gemini by up to 12 points on the MMVP benchmark. Additionally, the models demonstrate an average improvement of 3.06% across eight MLLM benchmarks, validating the dataset's ability to enhance both image difference recognition and overall visual understanding. The dataset exhibits high quality (over 70% accurate difference descriptions) and diversity (covering 1,203 object categories), offering valuable insights for multimodal data synthesis.</p>
</li>
<li class="task-list-item enabled">
<p><input class="task-list-item-checkbox"type="checkbox"> <strong>FlashSloth</strong>: Lightning Multimodal Large Language Models via Embedded Visual Compression. CVPR2025 <a href="https://arxiv.org/abs/2412.04317">Paper</a> <a href="https://github.com/codefanw/FlashSloth">Code</a></p>
</li>
<li class="task-list-item enabled">
<p><input class="task-list-item-checkbox"type="checkbox"> <strong>BlueLM-V-3B</strong>: Algorithm and System Co-Design for Multimodal Large Language Models on Mobile Devices. CVPR2025 <a href="https://arxiv.org/abs/2411.10640v1">Paper</a></p>
</li>
<li class="task-list-item enabled">
<p><input class="task-list-item-checkbox"type="checkbox"> <strong>Insight-V</strong>: Exploring Long-Chain Visual Reasoning with Multimodal Large Language Models. CVPR2025 <a href="https://arxiv.org/abs/2411.14432">Paper</a> <a href="https://github.com/dongyh20/Insight-V">Code</a></p>
</li>
<li class="task-list-item enabled">
<p><input class="task-list-item-checkbox"type="checkbox"> <strong>Critic-V</strong>: VLM Critics Help Catch VLM Errors in Multimodal Reasoning. CVPR2025 <a href="https://arxiv.org/abs/2411.18203">Paper</a></p>
</li>
<li class="task-list-item enabled">
<p><input class="task-list-item-checkbox"type="checkbox"> <strong>Mono-InternVL</strong>: Pushing the Boundaries of Monolithic Multimodal Large Language Models with Endogenous Visual Pre-training. CVPR2025 <a href="https://arxiv.org/abs/2410.08202">Paper</a> <a href="https://internvl.github.io/blog/2024-10-10-Mono-InternVL/">Code</a></p>
</li>
<li class="task-list-item enabled">
<p><input class="task-list-item-checkbox"type="checkbox"> <strong>DivPrune</strong>: Diversity-based Visual Token Pruning for Large Multimodal Models. CVPR2025 <a href="https://arxiv.org/abs/2503.02175">Paper</a> <a href="https://github.com/vbdi/divprune">Code</a></p>
</li>
<li class="task-list-item enabled">
<p><input class="task-list-item-checkbox"type="checkbox"> <strong>ODE</strong>: Open-Set Evaluation of Hallucinations in Multimodal Large Language Models. CVPR2025 <a href="https://arxiv.org/abs/2409.09318">Paper</a></p>
</li>
<li class="task-list-item enabled">
<p><input class="task-list-item-checkbox"type="checkbox"> Augmenting Multimodal LLMs with Self-Reflective Tokens for Knowledge-based Visual Question Answering. CVPR2025 <a href="https://arxiv.org/abs/2411.16863">Paper</a> <a href="https://github.com/aimagelab/ReflectiVA">Code</a></p>
</li>
<li class="task-list-item enabled">
<p><input class="task-list-item-checkbox"type="checkbox"> <strong>AGLA</strong>: Mitigating Object Hallucinations in Large Vision-Language Models with Assembly of Global and Local Attention. CVPR2025 <a href="https://arxiv.org/abs/2406.12718">Paper</a> <a href="https://github.com/Lackel/AGLA">Code</a></p>
</li>
<li class="task-list-item enabled">
<p><input class="task-list-item-checkbox"type="checkbox"> <strong>ICT</strong>: Image-Object Cross-Level Trusted Intervention for Mitigating Object Hallucination in Large Vision-Language Models. CVPR2025 <a href="https://arxiv.org/abs/2411.15268v1">Paper</a></p>
</li>
<li class="task-list-item enabled">
<p><input class="task-list-item-checkbox"type="checkbox"> Can Large Vision-Language Models Correct Grounding Errors By Themselves? CVPR2025 <a href="https://openreview.net/pdf?id=fO1xnmW8T6">Paper</a></p>
</li>
<li class="task-list-item enabled">
<p><input class="task-list-item-checkbox"type="checkbox"> <strong>Molmo and PixMo</strong>: Open Weights and Open Data for State-of-the-Art Vision-Language Models. CVPR2025 <a href="https://arxiv.org/abs/2409.17146">Paper</a> <a href="https://molmo.allenai.org/blog">Page</a></p>
</li>
<li class="task-list-item enabled">
<p><input class="task-list-item-checkbox"type="checkbox"> <strong>Nullu</strong>: Mitigating Object Hallucinations in Large Vision-Language Models via HalluSpace Projection. CVPR2025 <a href="https://arxiv.org/abs/2412.13817">Paper</a> <a href="https://github.com/Ziwei-Zheng/Nullu">Code</a></p>
</li>
<li class="task-list-item enabled">
<p><input class="task-list-item-checkbox"type="checkbox"> <strong>HiRes-LLaVA</strong>: Restoring Fragmentation Input in High-Resolution Large Vision-Language Models. CVPR2025 <a href="https://arxiv.org/abs/2407.08706">Paper</a></p>
</li>
<li class="task-list-item enabled">
<p><input class="task-list-item-checkbox"type="checkbox"> Devils in Middle Layers of Large Vision-Language Models: Interpreting, Detecting and Mitigating Object Hallucinations via Attention Lens. CVPR2025 <a href="https://arxiv.org/abs/2411.16724">Paper</a></p>
</li>
<li class="task-list-item enabled">
<p><input class="task-list-item-checkbox"type="checkbox"> <strong>HoVLE</strong>: Unleashing the Power of Monolithic Vision-Language Models with Holistic Vision-Language Embedding. CVPR2025 <a href="https://arxiv.org/abs/2412.16158">Paper</a></p>
</li>
<li class="task-list-item enabled">
<p><input class="task-list-item-checkbox"type="checkbox"> <strong>VoCo-LLaMA</strong>: Towards Vision Compression with Large Language Models. CVPR2025 <a href="https://arxiv.org/abs/2406.12275v2">Paper</a> <a href="https://github.com/Yxxxb/VoCo-LLaMA?tab=readme-ov-file">Code</a></p>
</li>
<li class="task-list-item enabled">
<p><input class="task-list-item-checkbox"type="checkbox"> Perception Tokens Enhance Visual Reasoning in Multimodal Language Models. CVPR2025 <a href="https://arxiv.org/abs/2412.03548">Paper</a> <a href="https://aurora-perception.github.io/">Page</a></p>
</li>
<li class="task-list-item enabled">
<p><input class="task-list-item-checkbox"type="checkbox"> <strong>Florence-VL</strong>: Enhancing Vision-Language Models with Generative Vision Encoder and Depth-Breadth Fusion. CVPR2025 <a href="https://arxiv.org/abs/2412.04424">Paper</a> <a href="https://jiuhaichen.github.io/florence-vl.github.io/">Page</a></p>
</li>
<li class="task-list-item enabled">
<p><input class="task-list-item-checkbox"type="checkbox"> <strong>ATP-LLaVA</strong>: Adaptive Token Pruning for Large Vision Language Models. CVPR2025 <a href="https://arxiv.org/abs/2412.00447">Paper</a> <a href="https://yxxxb.github.io/ATP-LLaVA-page/">Page</a></p>
</li>
<li class="task-list-item enabled">
<p><input class="task-list-item-checkbox"type="checkbox"> Accelerating Multimodel Large Language Models by Searching Optimal Vision Token Reduction. CVPR2025 <a href="https://arxiv.org/abs/2412.00556">Paper</a></p>
</li>
<li class="task-list-item enabled">
<p><input class="task-list-item-checkbox"type="checkbox"> <strong>VisionZip</strong>: Longer is Better but Not Necessary in Vision Language Models. CVPR2025 <a href="https://arxiv.org/abs/2412.04467">Paper</a> <a href="https://github.com/dvlab-research/VisionZip">Code</a></p>
</li>
</ul>

            
            
        </body>
        </html>